---
width: 1920
height: 1200
transition: slide
css:
 - chapter4/styles.css
---

## RAG dla Python Developer√≥w: Od Konceptu do Kodu üêç

**Czyli jak sprawiƒá, by LLM rozmawia≈Ç z Twoimi danymi (i nie halucynowa≈Ç!)**

---

## Po co nam RAG? (Problem do rozwiƒÖzania)

*   **LLM-y nie znajƒÖ Twoich prywatnych danych:** By≈Çy trenowane na ogromnych, ale *og√≥lnych* i czƒôsto *nieaktualnych* zbiorach danych (np. wiedza ko≈Ñczy siƒô w 2023).
*   **Halucynacje:** LLM-y potrafiƒÖ "wymy≈õlaƒá" odpowiedzi, gdy nie znajƒÖ fakt√≥w (lub gdy fakty sƒÖ sprzeczne z ich treningiem).
*   **Brak ≈∫r√≥de≈Ç:** Standardowe LLM-y rzadko podajƒÖ, skƒÖd wziƒô≈Çy informacje, co utrudnia weryfikacjƒô.

**RozwiƒÖzanie: Retrieval-Augmented Generation (RAG)**
Pozwalamy LLM-owi korzystaƒá z "otwartej ksiƒÖ≈ºki" - Twojej w≈Çasnej, aktualnej bazy wiedzy - podczas odpowiadania na pytania. ≈ÅƒÖczymy moc wyszukiwania informacji z mocƒÖ generowania jƒôzyka.

---

## RAG: Trzy Kluczowe Kroki - Koncept

Pomy≈õl o RAG jak o super-inteligentnym asystencie badawczym dla AI:

1.  **(R)etrieve - Znajd≈∫ Informacje:**
    *   **Cel:** Gdy przychodzi pytanie, znajdujemy *najbardziej trafne* fragmenty wiedzy w Twojej "bibliotece" (np. dokumentach firmowych, bazie danych, PDF-ach).
    *   **Jak?** Nie tylko po s≈Çowach kluczowych, ale g≈Ç√≥wnie **po znaczeniu** (wyszukiwanie semantyczne). System rozumie, ≈ºe "urlop zimowy" i "wyjazd na narty" to podobne tematy.

2.  **(A)ugment - Przygotuj "≈öciƒÖgawkƒô":**
    *   **Cel:** Znalezione informacje formatujemy i ≈ÇƒÖczymy z oryginalnym pytaniem, tworzƒÖc dla LLM-a idealnƒÖ "≈õciƒÖgawkƒô" (kontekst).
    *   **Przyk≈Çad:** Zamiast tylko pytaƒá LLM "Jaka jest cena produktu X?", dajemy mu: `Kontekst: [Fragment cennika: Produkt X kosztuje 100 PLN netto...] Pytanie: Jaka jest cena produktu X?`

3.  **(G)enerate - Wygeneruj Odpowied≈∫:**
    *   **Cel:** LLM (np. GPT-4, Gemini) dzia≈Ça jak 'autor' - na podstawie dostarczonej "≈õciƒÖgawki" generuje sp√≥jnƒÖ i trafnƒÖ odpowied≈∫, opartƒÖ na *dostarczonych* faktach.
    *   **Kontrola:** Mo≈ºemy wp≈Çywaƒá na styl odpowiedzi (np. bardziej faktograficzny vs kreatywny - `temperature`) i jej d≈Çugo≈õƒá (`max_tokens`).

---

## Poziom 2: Klocki RAG - Komponenty i Narzƒôdzia Python!

Co technicznie kryje siƒô za ka≈ºdym krokiem?

*   **Retrieve (Vector DB & Embeddings):**
    *   **Co:** Wyszukiwanie semantyczne w bazie wiedzy.
    *   **Embeddings (Reprezentacja Znaczenia):** Modele zamieniajƒÖce tekst na wektory (listy liczb).
        *   Narzƒôdzia: `openai` (API, np. `text-embedding-3-small`), `sentence-transformers` (modele lokalne/open-source np. `all-MiniLM-L6-v2`).
      *   **Vector DB (Przechowywanie i Wyszukiwanie Wektor√≥w):** Bazy zoptymalizowane pod kƒÖtem szybkiego wyszukiwania podobnych wektor√≥w (najbli≈ºszych sƒÖsiad√≥w KNN/ANN).
          *   Narzƒôdzia: `chromadb`, `faiss-cpu`/`faiss-gpu` (biblioteki), Pinecone (`pinecone-client`), Weaviate, Qdrant (bazy/us≈Çugi).
    *   *[Ikony: Vector DBs (Chroma, FAISS, Pinecone), SentenceTransformers]*

*   **Augment (Prompt Engineering):**
    *   **Co:** Tworzenie finalnego promptu dla LLM.
    *   **Techniki:**
        *   Proste ≈ÇƒÖczenie (Stuffing): `f"Kontekst: {context}\n\nPytanie: {query}"`. Dobre dla kr√≥tkich kontekst√≥w.
        *   Zaawansowane: Map-Reduce, Refine (obs≈Çuga wielu/d≈Çugich dokument√≥w, czƒôsto wspierane przez frameworki jak LangChain).

*   **Generate (LLM):**
    *   **Co:** Silnik jƒôzykowy generujƒÖcy odpowiedzi.
    *   **Narzƒôdzia Python:** `openai` (GPT-4/3.5), `google-generativeai` (Gemini), `transformers` (modele open-source jak Llama/Mistral), `ollama`, `vllm`.
    *   *[Ikony: Python, OpenAI, Google Cloud, Hugging Face]*

---

## Przygotowanie Bazy Wiedzy - PorzƒÖdkowanie "Biblioteki" AI

Zanim RAG zadzia≈Ça, musimy przygotowaƒá dane. To jak katalogowanie biblioteki:

1.  **Load:** Wczytaj dane z r√≥≈ºnych ≈∫r√≥de≈Ç (PDF, WWW, DB, TXT...).
    *   *Narzƒôdzia:* Loadery w LangChain/LlamaIndex, `requests`, `beautifulsoup4`, `pypdf`, `sqlalchemy` etc.
2.  **Chunking (Dzielenie na Fragmenty):** Podziel d≈Çugie dokumenty na mniejsze, logiczne czƒô≈õci (np. akapity, sekcje o sta≈Çej d≈Çugo≈õci). Kluczowe, by LLM dosta≈Ç precyzyjny kontekst.
    *   *Narzƒôdzia:* Splittery w LangChain/LlamaIndex (np. `RecursiveCharacterTextSplitter`), w≈Çasne skrypty.
3.  **Embeddings (Tworzenie Wektor√≥w):** Dla ka≈ºdego fragmentu wygeneruj wektor reprezentujƒÖcy jego znaczenie.
    *   *Narzƒôdzia:* Patrz komponent 'Retrieve'.
4.  **Index (Indeksowanie):** Zapisz fragmenty tekstu wraz z ich wektorami w bazie wektorowej, tworzƒÖc indeks umo≈ºliwiajƒÖcy szybkie wyszukiwanie.
    *   *Narzƒôdzia:* Patrz komponent 'Retrieve' (Vector DB).

*   **Wyzwanie:** Dob√≥r strategii chunkingu i modelu embedding√≥w ma du≈ºy wp≈Çyw na jako≈õƒá RAG!

---

## Architektura RAG: Przep≈Çyw Danych

Oto jak informacje przep≈ÇywajƒÖ w typowym systemie RAG:

```
+-------------------+      +----------------------+      +-------------------------+      +----------------+      +--------+
| Zapytanie         | ---> | Retrieve             | ---> | Augment                 | ---> | Generate (LLM) | ---> | Odp.   |
| (np. "Co to X?")  |      | (Szukaj w bazie      |      | (Prompt = Kontekst +    |      | (np. GPT-4,    |      |        |
|                   |      | wektorowej; chunking |      |         Zapytanie)      |      |  Gemini)       |      |        |
+-------------------+      |  & embedding         |      +-------------------------+      +----------------+      +--------+
                           |  na etapie prep.)    |       |
                           +----------------------+       | Znalezione fragmenty
                             ^                            v
                             |----------------------------| (Kontekst)
                             |
                       +----------------------+
                       | Baza Wiedzy          |
                       | (Wektorowa Baza:     |
                       |  Fragmenty + Wektory)|
                       +----------------------+
```

---

## Poziom 3: Szybka Implementacja z LangChain ü¶úüîó

LangChain upraszcza "sklejanie" komponent√≥w RAG. Zobaczmy kompletny ≈Ça≈Ñcuch:

```python
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain_core.documents import Document

# Za≈Ço≈ºenia:
# 1. `vectorstore`: Obiekt VectorStore za≈Çadowany danymi po chunkingu i embeddingu.
# 2. `embeddings`: Model embedding√≥w u≈ºyty do `vectorstore`, np. OpenAIEmbeddings().
vectorstore = InMemoryVectorStore(OpenAIEmbeddings(model="text-embedding-3-small"))
vectorstore.add_documents([Document(d) for d in ["Kot siedzi na drzewie.", "Pies biega po ≈ÇƒÖce.", "Samoch√≥d jedzie szybko."]])
# 3. `llm`: Zainicjalizowany model, np. ChatOpenAI(model="gpt-3.5-turbo").
llm = ChatOpenAI(model="gpt-3.5-turbo")

# Definicja Retrievera (interfejs do bazy wektorowej)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3}) # Pobierz Top 3 fragmenty

# Definicja szablonu Promptu
template = """Odpowiedz na pytanie bazujƒÖc TYLKO na poni≈ºszym kontek≈õcie:

{context}

Pytanie: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

# Funkcja formatujƒÖca pobrane dokumenty (obiekty LangChain Document) w jeden string
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Definicja ≈Åa≈Ñcucha RAG u≈ºywajƒÖc LangChain Expression Language (LCEL)
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()} # Pobierz i sformatuj kontekst, przeka≈º pytanie
    | prompt           # Wype≈Çnij szablon promptu
    | llm              # Wy≈õlij do LLM
    | StrOutputParser() # WyciƒÖgnij odpowied≈∫ jako string
)

# Wywo≈Çanie
question = "Jakie sƒÖ g≈Ç√≥wne cechy produktu X?"
response = rag_chain.invoke(question)
print(response)
```
*   `RunnablePassthrough()`: Przekazuje wej≈õcie (tutaj: pytanie) bez zmian.
*   `|` (pipe): ≈ÅƒÖczy komponenty w sekwencyjny pipeline.

---

## Poziom 3: LangChain - Przygotowanie Bazy Wektorowej (Krok 1: Retrieve Setup)

Jak stworzyƒá `vectorstore` u≈ºyty w poprzednim slajdzie?

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document # Lub u≈ºyj DocumentLoader√≥w

# 0. Za≈Çaduj dane (np. z pliku TXT, PDF etc. u≈ºywajƒÖc DocumentLoader√≥w LangChain)
# Za≈Ç√≥≈ºmy, ≈ºe masz listƒô string√≥w `texts` lub obiekt√≥w `Document`
texts = ["Jab≈Çka sƒÖ okrƒÖg≈Çe...", "Gruszki sƒÖ s≈Çodkie...", "...wiƒôcej o Pythonie..."]
docs = [Document(page_content=t) for t in texts] # Przyk≈Çadowe tworzenie obiekt√≥w Document

# 1. Chunking: Podziel dokumenty na mniejsze fragmenty
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
splits = text_splitter.split_documents(docs)

# 2. Wybierz model embedding√≥w
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# 3. Stw√≥rz Vector Store (np. Chroma) i zaindeksuj fragmenty
#    (Obliczy embeddingi dla `splits` i zapisze w bazie)
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="./chroma_db" # Opcjonalnie: zapisz na dysku
)

# Teraz `vectorstore` jest gotowy do u≈ºycia w `rag_chain`
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# Test retrievera
retrieved_docs = retriever.invoke("Opowiedz mi o owocach")
print(f"Pobrano {len(retrieved_docs)} fragment√≥w.")
# Mo≈ºna sprawdziƒá zawarto≈õƒá retrieved_docs[0].page_content
```

---

## Poziom 4: Zajrzyjmy pod Maskƒô - Implementacja 'Raw' w Pythonie üõ†Ô∏è

A jak zrobiƒá RAG bez LangChain? U≈ºywajƒÖc podstawowych bibliotek:

**Krok 1: Retrieve (Embeddings + Wyszukiwanie Wektorowe z NumPy)**

```python
import openai
import numpy as np

# Konfiguracja API (za≈Ç√≥≈ºmy ustawione zmienne ≈õrodowiskowe)
# import os
# openai.api_key = os.getenv("OPENAI_API_KEY")

# --- Funkcja do generowania Embedding√≥w ---
def get_embedding(text, model="text-embedding-3-small"):
   text = text.replace("\n", " ")
   try:
       response = openai.embeddings.create(input=[text], model=model)
       return response.data[0].embedding # Zwraca listƒô float
   except Exception as e:
       print(f"B≈ÇƒÖd embeddingu dla: {text[:50]}... Error: {e}")
       return None

# --- Przyk≈Çadowe Dane (po chunkingu!) ---
# W praktyce te fragmenty pochodzƒÖ z etapu przygotowania bazy wiedzy
document_chunks = [
    "Jab≈Çka sƒÖ zazwyczaj okrƒÖg≈Çe i rosnƒÖ na drzewach jab≈Çoni.",
    "Gruszki majƒÖ charakterystyczny kszta≈Çt, czƒôsto wyd≈Çu≈ºony u do≈Çu.",
    "Python jest jƒôzykiem programowania interpretowanym, wysokiego poziomu.",
    "Czerwone jab≈Çka odmiany 'Ligol' sƒÖ znane ze swojej chrupko≈õci."
]

# --- Wygeneruj i przechowaj embeddingi (w pamiƒôci dla przyk≈Çadu) ---
# W realnym systemie: u≈ºyj FAISS, ChromaDB API, Pinecone API itp.
docs_data = []
for chunk in document_chunks:
    embedding_list = get_embedding(chunk)
    if embedding_list:
        docs_data.append({
            "text": chunk,
            # Konwertuj na numpy array float32 dla wydajno≈õci oblicze≈Ñ
            "embedding": np.array(embedding_list, dtype=np.float32)
        })

# --- Wyszukiwanie Podobie≈Ñstwa Kosinusowego ---
def cosine_similarity(v1, v2):
    dot_product = np.dot(v1, v2)
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)
    if norm_v1 == 0 or norm_v2 == 0: return 0.0
    return dot_product / (norm_v1 * norm_v2)

# --- Znajd≈∫ Top-K najbardziej podobnych fragment√≥w ---
user_query = "Opowiedz mi o kszta≈Çcie owoc√≥w"
query_embedding_list = get_embedding(user_query)

if query_embedding_list:
    query_vector = np.array(query_embedding_list, dtype=np.float32)

    # Oblicz podobie≈Ñstwo do wszystkich fragment√≥w w bazie
    scores = []
    for item in docs_data:
        sim = cosine_similarity(query_vector, item["embedding"])
        scores.append({"text": item["text"], "score": sim})

    # Sortuj i wybierz top K
    top_k = 2
    results = sorted(scores, key=lambda x: x["score"], reverse=True)[:top_k]

    # Stw√≥rz kontekst z najlepszych wynik√≥w
    context = "\n\n".join([res["text"] for res in results])

    print(f"--- Pobrany Kontekst (Top {top_k}) ---")
    print(context)
else:
    print("Nie uda≈Ço siƒô uzyskaƒá embeddingu dla zapytania.")
    context = "" # Ustaw pusty kontekst w razie b≈Çƒôdu
```

---

## Poziom 4: Implementacja 'Raw' - Kroki 2 & 3 (Augment + Generate)

MajƒÖc `context` z poprzedniego kroku, rƒôcznie tworzymy prompt i wywo≈Çujemy API LLM:

```python
import openai # Upewnij siƒô, ≈ºe skonfigurowane

# --- Krok 2: Augment (Rƒôczne Tworzenie Promptu) ---
# U≈ºywamy `context` i `user_query` z poprzednich krok√≥w

if context: # Sprawd≈∫, czy kontekst zosta≈Ç znaleziony
    final_prompt = f"""Na podstawie poni≈ºszego kontekstu odpowiedz na pytanie. Odpowiadaj tylko informacjami zawartymi w kontek≈õcie.

Kontekst:
---
{context}
---

Pytanie: {user_query}

Odpowied≈∫:"""
else: # Fallback, je≈õli nie znaleziono kontekstu
    final_prompt = f"Odpowiedz na pytanie: {user_query}"
    print("INFO: Nie znaleziono relevantnego kontekstu, wysy≈Çam samo pytanie do LLM.")


print("\n--- Finalny Prompt dla LLM ---")
print(final_prompt)


# --- Krok 3: Generate (Bezpo≈õrednie Wywo≈Çanie API LLM) ---
def call_llm_api(prompt_to_send):
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo", # Lub inny model
            messages=[
                {"role": "system", "content": "Jeste≈õ pomocnym asystentem. Odpowiadaj na pytania u≈ºytkownika. Je≈õli dostarczono kontekst, opieraj odpowied≈∫ g≈Ç√≥wnie na nim."},
                {"role": "user", "content": prompt_to_send}
            ],
            temperature=0.1 # Niska temperatura dla bardziej faktograficznych odpowiedzi
        )
        answer = response.choices[0].message.content
        return answer
    except Exception as e:
        print(f"B≈ÇƒÖd podczas wywo≈Çania API LLM: {e}")
        return "Przepraszam, wystƒÖpi≈Ç b≈ÇƒÖd."

# Wywo≈Çanie LLM
final_answer = call_llm_api(final_prompt)

print("\n--- Ostateczna Odpowied≈∫ LLM ---")
print(final_answer)
```

---

## Praktyczny Aspekt: Co gdy RAG nie wie?

Co je≈õli zapytasz chatbota RAG o pogodƒô, a jego baza wiedzy zawiera tylko dane o produktach firmy?

*   **Dobra praktyka:** System powinien to rozpoznaƒá i odpowiedzieƒá grzecznie, zamiast halucynowaƒá.
    *   ‚úÖ "Przepraszam, nie mam informacji o pogodzie. Mogƒô jednak odpowiedzieƒá na pytania dotyczƒÖce naszych produkt√≥w X, Y, Z."
    *   ‚ùå Unikamy: "Wed≈Çug moich danych, pogoda jest s≈Çoneczna." (gdy nie ma danych) lub "B≈ÇƒÖd systemu."
*   **Jak to zaimplementowaƒá?**
    1.  **Sprawd≈∫ wynik Retrieve:** Je≈õli `Retrieve` nie zwr√≥ci≈Ço ≈ºadnych sensownych fragment√≥w (niski `score` podobie≈Ñstwa), to sygna≈Ç, ≈ºe pytanie jest poza zakresem bazy wiedzy.
    2.  **Dodatkowa klasyfikacja:** Mo≈ºna u≈ºyƒá prostego LLM call lub klasyfikatora do oceny, czy pytanie dotyczy domeny bazy wiedzy.
    3.  **Fallback Prompt:** Je≈õli pytanie jest poza zakresem, u≈ºyj predefiniowanej odpowiedzi lub innego promptu dla LLM, kt√≥ry instruuje go, jak grzecznie odm√≥wiƒá odpowiedzi.
*   **CiƒÖg≈Çe ulepszanie:** Loguj pytania, na kt√≥re RAG nie znalaz≈Ç odpowiedzi - to cenne ≈∫r√≥d≈Ço informacji, jak rozszerzyƒá bazƒô wiedzy!

---

## Podsumowanie

*   **Wiarygodno≈õƒá:** LLM opiera odpowiedzi na *konkretnych, dostarczonych* danych, a nie tylko na swojej wewnƒôtrznej (czasem b≈Çƒôdnej lub nieaktualnej) wiedzy.
*   **Aktualno≈õƒá i Specjalizacja:** Pozwala LLM "nauczyƒá siƒô" Twojej firmowej, dziedzinowej lub po prostu najnowszej wiedzy bez kosztownego re-treningu.
*   **Redukcja Halucynacji:** ZnaczƒÖco ogranicza tendencjƒô LLM do wymy≈õlania fakt√≥w.
*   **Mo≈ºliwo≈õƒá Weryfikacji:** Mo≈ºna (i warto!) pokazywaƒá u≈ºytkownikowi ≈∫r√≥d≈Ça (pobrane fragmenty), na podstawie kt√≥rych powsta≈Ça odpowied≈∫.
*   **Tworzenie U≈ºytecznych Aplikacji:** Umo≈ºliwia budowƒô chatbot√≥w, system√≥w Q&A, asystent√≥w, kt√≥re faktycznie rozwiƒÖzujƒÖ problemy w oparciu o specyficzne dane.

**Kluczowe Narzƒôdzia Python:** `openai`, `google-generativeai`, `LangChain`, `LlamaIndex`, `sentence-transformers`, `chromadb`, `faiss`, `pinecone-client`, `numpy`, `requests`...

---

## The end!