---
width: 1920
height: 1200
transition: slide
css:
 - chapter4/styles.css
---

## RAG dla Python Developer贸w: Od Konceptu do Kodu 

**Czyli jak sprawi, by LLM rozmawia z Twoimi danymi (i nie halucynowa!)**

---

## Po co nam RAG? (Problem do rozwizania)

Jak sprawi, 偶eby LLM poznaTw贸j problem. 

Przykdowy problem. Mamy firm, w niej s HR, kt贸re maj wasne procedury i reguy. Jak zautomatyzowa proces odpowiedzi na pytania dotyczce tych procedur. 

Odpowiedzi jest RAG, bo dostarczam wiedz uszyt na moj potrzeb, buduj kontekst i generuje odpowied藕 w jzyku naturalnym odpowiednio sformatowan.

Zagro偶enia:
*   **LLM-y nie znaj Twoich prywatnych danych:** Byy trenowane na ogromnych, ale *og贸lnych* i czsto *nieaktualnych* zbiorach danych (np. wiedza koczy si w 2023).
*   **Halucynacje:** LLM-y potrafi "wymyla" odpowiedzi, gdy nie znaj fakt贸w (lub gdy fakty s sprzeczne z ich treningiem).
*   **Brak 藕r贸de:** Standardowe LLM-y rzadko podaj, skd wziy informacje, co utrudnia weryfikacj. (a co z perplexity)

**Rozwizanie: Retrieval-Augmented Generation (RAG)**
Pozwalamy LLM-owi korzysta z "otwartej ksi偶ki" - Twojej wasnej, aktualnej bazy wiedzy - podczas odpowiadania na pytania. czymy moc wyszukiwania informacji z moc generowania jzyka.

---

## Poziom - koncept = RAG: Trzy Kluczowe Kroki

Pomyl o RAG jak o super-inteligentnym asystencie badawczym dla AI:

1.  **(R)etrieve - Znajd藕 Informacje:**
    *   **Cel:** Gdy przychodzi pytanie, znajdujemy *najbardziej trafne* fragmenty wiedzy w Twojej "bibliotece" (np. dokumentach firmowych, bazie danych, PDF-ach).
    *   **Jak?** Nie tylko po sowach kluczowych, ale g贸wnie **po znaczeniu** (wyszukiwanie semantyczne). System rozumie, 偶e "urlop zimowy" i "wyjazd na narty" to podobne tematy.

2.  **(A)ugment - Przygotuj "cigawk":**
    *   **Cel:** Znalezione informacje formatujemy i czymy z oryginalnym pytaniem, tworzc dla LLM-a idealn "cigawk" (kontekst).
    *   **Przykad:** Zamiast tylko pyta LLM "Jaka jest cena produktu X?", dajemy mu: `Kontekst: [Fragment cennika: Produkt X kosztuje 100 PLN netto...] Pytanie: Jaka jest cena produktu X?`

3.  **(G)enerate - Wygeneruj Odpowied藕:**
    *   **Cel:** LLM (np. GPT-4, Gemini) dziaa jak 'autor' - na podstawie dostarczonej "cigawki" generuje sp贸jn i trafn odpowied藕, opart na *dostarczonych* faktach.
    *   **Kontrola:** Mo偶emy wpywa na styl odpowiedzi (np. bardziej faktograficzny vs kreatywny - `temperature`) i jej dugo (`max_tokens`).

diagram architektury

---
Tak wyglda najprostrszy RAG. najpierw raw (Bez langchaina).

kod w pythonie

---

## Poziom - implementacja: Retrieve

Co technicznie kryje si za ka偶dym krokiem?

*   **Retrieve (Vector DB & Embeddings):**
    *   **Co:** Wyszukiwanie semantyczne w bazie wiedzy.
    *   **Embeddings (Reprezentacja Znaczenia):** Modele zamieniajce tekst na wektory (listy liczb).
        *   Narzdzia: `openai` (API, np. `text-embedding-3-small`), `sentence-transformers` (modele lokalne/open-source np. `all-MiniLM-L6-v2`).
      *   **Vector DB (Przechowywanie i Wyszukiwanie Wektor贸w):** Bazy zoptymalizowane pod ktem szybkiego wyszukiwania podobnych wektor贸w (najbli偶szych ssiad贸w KNN/ANN).
          *   Narzdzia: `chromadb`, `faiss-cpu`/`faiss-gpu` (biblioteki), Pinecone (`pinecone-client`), Weaviate, Qdrant (bazy/usugi).
    *   *[Ikony: Vector DBs (Chroma, FAISS, Pinecone), SentenceTransformers]*

Kod w pythonie

---

## Poziom - implementacja: Augument

*   **Augment (Prompt Engineering):**
    *   **Co:** Tworzenie finalnego promptu dla LLM.
    *   **Techniki:**
        *   Proste czenie (Stuffing): `f"Kontekst: {context}\n\nPytanie: {query}"`. Dobre dla kr贸tkich kontekst贸w.
        *   Zaawansowane: Map-Reduce, Refine (obsuga wielu/dugich dokument贸w, czsto wspierane przez frameworki jak LangChain).

kod w pythonie

---
## Poziom - implementacja: Generate

*   **Generate (LLM):**
    *   **Co:** Silnik jzykowy generujcy odpowiedzi.
    *   **Narzdzia Python:** `openai` (GPT-4/3.5), `google-generativeai` (Gemini), `transformers` (modele open-source jak Llama/Mistral), `ollama`, `vllm`.
    *   *[Ikony: Python, OpenAI, Google Cloud, Hugging Face]*

kod w pythonie

---

Ponownie najprostrzy RAG, ale langchain.

---
aws, google cloud?

streamlit?

---
tank you

notes: 

Najlepsze praktyki:
- langchain
- langfuse jako observability i prompt manager
- endpointy. id konwersacji. 
